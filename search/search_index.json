{"config":{"lang":["ja","en"],"separator":"[\\s\\-\u3000\u3001\u3002\uff0c\uff0e]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"toiro \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8","text":"<p>toiro\uff08\u3068\u3044\u308d\uff09\u306f \u65e5\u672c\u8a9e\u306e\u5404\u7a2e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6 \u3092\u6bd4\u8f03\u3059\u308b\u305f\u3081\u306e Python \u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u3059\u3002 \u4ee5\u4e0b\u306e\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <ul> <li>\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u306e \u51e6\u7406\u901f\u5ea6\u3092\u6bd4\u8f03</li> <li>\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3054\u3068\u306e \u5206\u304b\u3061\u66f8\u304d\u7d50\u679c\u3092\u6bd4\u8f03</li> <li>\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u30bf\u30b9\u30af\uff08\u4f8b: \u30c6\u30ad\u30b9\u30c8\u5206\u985e\uff09\u3067\u306e\u6027\u80fd\u6bd4\u8f03</li> <li>\u65e5\u672c\u8a9e NLP \u306e\u88dc\u52a9\u6a5f\u80fd\uff08\u30b3\u30fc\u30d1\u30b9\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9/\u524d\u51e6\u7406\u3001\u7c21\u6613\u5206\u985e\u5668 \u306a\u3069\uff09</li> </ul> <p></p>"},{"location":"#_1","title":"\u4e3b\u306a\u6a5f\u80fd","text":""},{"location":"#_2","title":"\u5bfe\u5fdc\u30c8\u30fc\u30af\u30ca\u30a4\u30b6","text":"<p>13\u7a2e\u985e\u306e\u65e5\u672c\u8a9e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3068 BPE \u3092\u30b5\u30dd\u30fc\u30c8\uff1a</p> <ul> <li>janome\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\u642d\u8f09\uff09</li> <li>nagisa</li> <li>mecab-python3</li> <li>sudachipy</li> <li>spacy</li> <li>ginza</li> <li>kytea</li> <li>jumanpp</li> <li>sentencepiece</li> <li>fugashi (ipadic/unidic)</li> <li>tinysegmenter</li> <li>tiktoken (GPT-4o / GPT-5 \u7528 BPE)</li> </ul>"},{"location":"#_3","title":"\u30ea\u30f3\u30af","text":"<p>\ud83d\udc49 \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u672c\u4f53: https://github.com/taishi-i/toiro \ud83d\udc49 \u30c7\u30e2\uff08Hugging Face Spaces\uff09: https://huggingface.co/spaces/taishi-i/Japanese-Tokenizer-Comparison \ud83d\udc49 PyPI: https://pypi.org/project/toiro/</p> <p>\u5bfe\u5fdc Python \u30d0\u30fc\u30b8\u30e7\u30f3</p> <p>Python 3.10 \u4ee5\u4e0a\u3092\u63a8\u5968\u3002</p> <p>\u30e9\u30a4\u30bb\u30f3\u30b9</p> <p>toiro \u306f Apache License 2.0 \u306e\u4e0b\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <p>\u3053\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u30b5\u30a4\u30c8\u306f MkDocs Material \u306b\u3088\u3063\u3066\u751f\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"api/","title":"API \u30ea\u30d5\u30a1\u30ec\u30f3\u30b9","text":"<p>\u4ee5\u4e0b\u306f\u81ea\u52d5\u751f\u6210\u3055\u308c\u305f API \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u3059\u3002</p>"},{"location":"benchmarks/","title":"\u30d9\u30f3\u30c1\u30de\u30fc\u30af","text":"<p>toiro \u306f\u4ee5\u4e0b\u306e3\u3064\u306e\u89b3\u70b9\u3067\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3092\u6bd4\u8f03\u3059\u308b\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p>"},{"location":"benchmarks/#1","title":"1. \u51e6\u7406\u901f\u5ea6\u306e\u6bd4\u8f03","text":"<p>\u8907\u6570\u306e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3067\u540c\u3058\u30c6\u30ad\u30b9\u30c8\u3092\u51e6\u7406\u3057\u3001\u5b9f\u884c\u6642\u9593\u3092\u8a08\u6e2c\u3057\u307e\u3059\u3002</p> <pre><code>from toiro import tokenizers\nfrom toiro import datadownloader\n\n# \u30b3\u30fc\u30d1\u30b9\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\ndatadownloader.download_corpus(\"livedoor_news_corpus\")\ntrain_df, dev_df, test_df = datadownloader.load_corpus(\"livedoor_news_corpus\")\ntexts = train_df[1].tolist()\n\n# \u901f\u5ea6\u6bd4\u8f03\u3092\u5b9f\u884c\nreport = tokenizers.compare(texts)\nprint(report)\n</code></pre> <p>\u51fa\u529b\u4f8b: <pre><code>{\n  'execution_environment': {\n    'python_version': '3.10.0',\n    'arch': 'X86_64',\n    'brand_raw': 'Intel(R) Core(TM) i7-7700K CPU @ 4.20GHz',\n    'count': 8\n  },\n  'data': {\n    'number_of_sentences': 5900,\n    'average_length': 37.69\n  },\n  'janome': {'elapsed_time': 9.11},\n  'nagisa': {'elapsed_time': 15.87},\n  'sudachipy': {'elapsed_time': 9.05}\n}\n</code></pre></p>"},{"location":"benchmarks/#2","title":"2. \u5206\u5272\u7d50\u679c\u306e\u6bd4\u8f03","text":"<p>\u540c\u4e00\u30c6\u30ad\u30b9\u30c8\u306b\u5bfe\u3057\u3066\u5404\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u304c\u3069\u306e\u3088\u3046\u306b\u5206\u5272\u3059\u308b\u304b\u3092\u8996\u899a\u7684\u306b\u6bd4\u8f03\u3057\u307e\u3059\u3002</p> <pre><code>from toiro import tokenizers\n\ntext = \"\u90fd\u5e81\u6240\u5728\u5730\u306f\u65b0\u5bbf\u533a\u3002\"\ntokenizers.print_words(text, delimiter=\"|\")\n</code></pre> <p>\u51fa\u529b\u4f8b: <pre><code>       janome: \u90fd\u5e81|\u6240\u5728\u5730|\u306f|\u65b0\u5bbf|\u533a|\u3002\n       nagisa: \u90fd\u5e81|\u6240\u5728|\u5730|\u306f|\u65b0\u5bbf|\u533a|\u3002\n    sudachipy: \u90fd\u5e81|\u6240\u5728\u5730|\u306f|\u65b0\u5bbf\u533a|\u3002\n</code></pre></p>"},{"location":"benchmarks/#_2","title":"\u3088\u308a\u8a73\u7d30\u306a\u6bd4\u8f03","text":"<p>\u7279\u5b9a\u306e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3092\u9078\u629e\u3057\u3066\u3001\u8a73\u7d30\u306a\u5206\u6790\u3092\u884c\u3046\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\uff1a</p> <pre><code>from toiro.tokenizers import SelectTokenizer\n\n# \u7279\u5b9a\u306e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3092\u9078\u629e\ntokenizer = SelectTokenizer(\"sudachipy\")\ntokens = tokenizer.tokenize(\"\u90fd\u5e81\u6240\u5728\u5730\u306f\u65b0\u5bbf\u533a\u3002\")\nprint(tokens)\n</code></pre>"},{"location":"benchmarks/#3","title":"3. \u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u30bf\u30b9\u30af\u3067\u306e\u6027\u80fd\u6bd4\u8f03","text":"<p>\u5b9f\u969b\u306e\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u30bf\u30b9\u30af\uff08\u4f8b: \u30c6\u30ad\u30b9\u30c8\u5206\u985e\uff09\u3067\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u306e\u6027\u80fd\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002</p>"},{"location":"benchmarks/#svm","title":"SVM \u3092\u4f7f\u3063\u305f\u5206\u985e","text":"<pre><code>from toiro import tokenizers, datadownloader\nfrom toiro.classifiers import SVMClassificationModel\n\n# \u30c7\u30fc\u30bf\u306e\u6e96\u5099\ntrain_df, dev_df, test_df = datadownloader.load_corpus(\"livedoor_news_corpus\")\n\n# \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3092\u9078\u629e\ntokenizer = tokenizers.SelectTokenizer(\"janome\")\n\n# SVM \u30e2\u30c7\u30eb\u3067\u5b66\u7fd2\u30fb\u8a55\u4fa1\nmodel = SVMClassificationModel(tokenizer)\nmodel.train(train_df)\nmetrics = model.evaluate(test_df)\nprint(metrics)\n</code></pre>"},{"location":"benchmarks/#bert","title":"BERT \u3092\u4f7f\u3063\u305f\u5206\u985e","text":"<pre><code>from toiro.classifiers import BERTClassificationModel\n\n# BERT \u30e2\u30c7\u30eb\u3067\u5b66\u7fd2\u30fb\u8a55\u4fa1\uff08\u8981: torch, transformers, catalyst\uff09\nmodel = BERTClassificationModel(\"cl-tohoku/bert-base-japanese\")\nmodel.train(train_df, dev_df, epochs=3)\nmetrics = model.evaluate(test_df)\nprint(metrics)\n</code></pre>"},{"location":"benchmarks/#_3","title":"\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u7d50\u679c\u306e\u4fdd\u5b58","text":"<p>\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u76f4\u63a5\u6bd4\u8f03\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\uff1a</p> <pre><code>from toiro import tokenizers\n\n# \u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8aad\u307f\u8fbc\u3093\u3067\u6bd4\u8f03\nreport = tokenizers.compare_from_file(\"path/to/textfile.txt\")\nprint(report)\n</code></pre> <p>\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u306e\u30d2\u30f3\u30c8</p> <ul> <li>\u5927\u91cf\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u51e6\u7406\u3059\u308b\u5834\u5408\u3001MeCab\u3001fugashi\u3001SudachiPy \u304c\u9ad8\u901f\u3067\u3059</li> <li>\u30e1\u30e2\u30ea\u52b9\u7387\u3092\u91cd\u8996\u3059\u308b\u5834\u5408\u306f Janome \u3084 TinySegmenter \u304c\u304a\u3059\u3059\u3081\u3067\u3059</li> <li>\u7cbe\u5ea6\u91cd\u8996\u306e\u5834\u5408\u306f GiNZA \u3084 BERT \u30d9\u30fc\u30b9\u306e\u30e2\u30c7\u30eb\u3092\u691c\u8a0e\u3057\u3066\u304f\u3060\u3055\u3044</li> </ul> <p>\u8a73\u7d30\u306a\u4f8b</p> <p>\u3088\u308a\u8a73\u3057\u3044\u4f7f\u7528\u4f8b\u306f\u3001GitHub \u30ea\u30dd\u30b8\u30c8\u30ea\u306e examples \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"contributing/","title":"\u958b\u767a\u30fb\u8ca2\u732e\u30ac\u30a4\u30c9","text":"<p>toiro \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3078\u306e\u8ca2\u732e\u3092\u6b53\u8fce\u3057\u307e\u3059\uff01\u30d0\u30b0\u5831\u544a\u3001\u6a5f\u80fd\u8981\u671b\u3001\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u306a\u3069\u3001\u3042\u3089\u3086\u308b\u5f62\u3067\u306e\u8ca2\u732e\u3092\u304a\u5f85\u3061\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"contributing/#_2","title":"\u884c\u52d5\u898f\u7bc4\u3068\u30ac\u30a4\u30c9\u30e9\u30a4\u30f3","text":"<ul> <li>\u884c\u52d5\u898f\u7bc4: CODE_OF_CONDUCT.md</li> <li>\u30b3\u30f3\u30c8\u30ea\u30d3\u30e5\u30fc\u30c8\u30ac\u30a4\u30c9: CONTRIBUTING.md</li> </ul>"},{"location":"contributing/#_3","title":"\u958b\u767a\u74b0\u5883\u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7","text":""},{"location":"contributing/#1","title":"1. \u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u30af\u30ed\u30fc\u30f3","text":"<pre><code>git clone https://github.com/taishi-i/toiro.git\ncd toiro\n</code></pre>"},{"location":"contributing/#2","title":"2. \u4eee\u60f3\u74b0\u5883\u306e\u4f5c\u6210\u3068\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30c8","text":"<pre><code>python -m venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\n</code></pre>"},{"location":"contributing/#3","title":"3. \u958b\u767a\u7528\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<pre><code>pip install -U pip\npip install -e .\npip install -e \".[all_tokenizers]\"  # \u5168\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09\n</code></pre>"},{"location":"contributing/#4","title":"4. \u30c6\u30b9\u30c8\u306e\u5b9f\u884c","text":"<pre><code>pytest -q  # \u307e\u305f\u306f: python -m pytest\n</code></pre> <p>\u30c6\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u306f <code>test/</code> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u914d\u7f6e\u3055\u308c\u3066\u3044\u307e\u3059\uff1a - <code>test_tokenizers.py</code> - \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u306e\u30c6\u30b9\u30c8 - <code>test_datadownloader.py</code> - \u30c7\u30fc\u30bf\u30c0\u30a6\u30f3\u30ed\u30fc\u30c0\u306e\u30c6\u30b9\u30c8 - <code>test_classifiers.py</code> - \u5206\u985e\u5668\u306e\u30c6\u30b9\u30c8</p>"},{"location":"contributing/#_4","title":"\u8ca2\u732e\u306e\u65b9\u6cd5","text":""},{"location":"contributing/#_5","title":"\u30d0\u30b0\u5831\u544a","text":"<p>GitHub Issues \u3067\u30d0\u30b0\u3092\u5831\u544a\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4ee5\u4e0b\u306e\u60c5\u5831\u3092\u542b\u3081\u308b\u3068\u52a9\u304b\u308a\u307e\u3059\uff1a - Python \u30d0\u30fc\u30b8\u30e7\u30f3 - toiro \u30d0\u30fc\u30b8\u30e7\u30f3 - \u518d\u73fe\u624b\u9806 - \u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8</p>"},{"location":"contributing/#_6","title":"\u6a5f\u80fd\u8981\u671b","text":"<p>\u65b0\u6a5f\u80fd\u306e\u30a2\u30a4\u30c7\u30a2\u304c\u3042\u308b\u5834\u5408\u306f\u3001GitHub Issues \u3067\u63d0\u6848\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"contributing/#_7","title":"\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8","text":"<ol> <li>\u3053\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u30d5\u30a9\u30fc\u30af</li> <li>\u65b0\u3057\u3044\u30d6\u30e9\u30f3\u30c1\u3092\u4f5c\u6210 (<code>git checkout -b feature/your-feature</code>)</li> <li>\u5909\u66f4\u3092\u30b3\u30df\u30c3\u30c8 (<code>git commit -am 'Add some feature'</code>)</li> <li>\u30d6\u30e9\u30f3\u30c1\u306b\u30d7\u30c3\u30b7\u30e5 (<code>git push origin feature/your-feature</code>)</li> <li>\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u4f5c\u6210</li> </ol>"},{"location":"contributing/#_8","title":"\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u306e\u8ffd\u52a0","text":"<p>\u65b0\u3057\u3044\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3092\u8ffd\u52a0\u3059\u308b\u5834\u5408\uff1a</p> <ol> <li><code>toiro/tokenizers/</code> \u306b\u65b0\u3057\u3044\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\uff08\u4f8b: <code>tokenizer_newone.py</code>\uff09</li> <li><code>tokenize()</code> \u95a2\u6570\u3092\u5b9f\u88c5</li> <li><code>tokenizer_utils.py</code> \u306b\u53ef\u7528\u6027\u30c1\u30a7\u30c3\u30af\u95a2\u6570\u3092\u8ffd\u52a0</li> <li><code>__init__.py</code> \u3092\u66f4\u65b0\u3057\u3066\u65b0\u3057\u3044\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3092\u30a4\u30f3\u30dd\u30fc\u30c8</li> <li>\u30c6\u30b9\u30c8\u3092 <code>test/test_tokenizers.py</code> \u306b\u8ffd\u52a0</li> </ol>"},{"location":"contributing/#_9","title":"\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u66f4\u65b0","text":"<p>\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306f <code>docs/</code> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u3042\u308a\u307e\u3059\u3002MkDocs \u3092\u4f7f\u7528\u3057\u3066\u30ed\u30fc\u30ab\u30eb\u3067\u30d7\u30ec\u30d3\u30e5\u30fc\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>pip install mkdocs-material mkdocs-static-i18n mkdocstrings[python]\nmkdocs serve\n</code></pre> <p>\u30d6\u30e9\u30a6\u30b6\u3067 <code>http://127.0.0.1:8000</code> \u3092\u958b\u3044\u3066\u30d7\u30ec\u30d3\u30e5\u30fc\u3092\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"contributing/#_10","title":"\u8cea\u554f\u3084\u30b5\u30dd\u30fc\u30c8","text":"<p>\u8cea\u554f\u304c\u3042\u308b\u5834\u5408\u306f\u3001GitHub Discussions \u307e\u305f\u306f Issues \u3067\u304a\u6c17\u8efd\u306b\u304a\u5c0b\u306d\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"corpora/","title":"\u30b3\u30fc\u30d1\u30b9\u3068\u30c7\u30fc\u30bf","text":"<p><code>toiro.datadownloader</code> \u30e2\u30b8\u30e5\u30fc\u30eb\u306f\u3001\u65e5\u672c\u8a9e\u306e\u30c6\u30ad\u30b9\u30c8\u5206\u985e\u30bf\u30b9\u30af\u7528\u306e\u30b3\u30fc\u30d1\u30b9\u3092\u7c21\u5358\u306b\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u5229\u7528\u3067\u304d\u308b\u6a5f\u80fd\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p>"},{"location":"corpora/#_2","title":"\u5229\u7528\u53ef\u80fd\u306a\u30b3\u30fc\u30d1\u30b9","text":""},{"location":"corpora/#livedoor","title":"livedoor \u30cb\u30e5\u30fc\u30b9\u30b3\u30fc\u30d1\u30b9","text":"<ul> <li>\u30bf\u30b9\u30af: \u30c6\u30ad\u30b9\u30c8\u5206\u985e\uff08\u30cb\u30e5\u30fc\u30b9\u30ab\u30c6\u30b4\u30ea\u5206\u985e\uff09</li> <li>\u30ab\u30c6\u30b4\u30ea\u6570: 9\u30ab\u30c6\u30b4\u30ea</li> <li>\u30ab\u30c6\u30b4\u30ea: \u30c8\u30d4\u30c3\u30af\u30cb\u30e5\u30fc\u30b9\u3001Sports Watch\u3001IT\u30e9\u30a4\u30d5\u30cf\u30c3\u30af\u3001\u5bb6\u96fb\u30c1\u30e3\u30f3\u30cd\u30eb\u3001MOVIE ENTER\u3001\u72ec\u5973\u901a\u4fe1\u3001\u30a8\u30b9\u30de\u30c3\u30af\u30b9\u3001livedoor HOMME\u3001Peachy</li> <li>\u63d0\u4f9b\u5143: livedoor \u30cb\u30e5\u30fc\u30b9\u30b3\u30fc\u30d1\u30b9</li> </ul>"},{"location":"corpora/#yahoo","title":"Yahoo! \u6620\u753b\u30ec\u30d3\u30e5\u30fc","text":"<ul> <li>\u30bf\u30b9\u30af: \u611f\u60c5\u5206\u6790\uff08\u30dd\u30b8\u30c6\u30a3\u30d6/\u30cd\u30ac\u30c6\u30a3\u30d6\uff09</li> <li>\u30c9\u30e1\u30a4\u30f3: \u6620\u753b\u30ec\u30d3\u30e5\u30fc</li> <li>\u30c7\u30fc\u30bf\u5f62\u5f0f: \u30ec\u30d3\u30e5\u30fc\u30c6\u30ad\u30b9\u30c8\u3068\u8a55\u4fa1\u30b9\u30b3\u30a2</li> </ul>"},{"location":"corpora/#amazon","title":"Amazon \u30ec\u30d3\u30e5\u30fc","text":"<ul> <li>\u30bf\u30b9\u30af: \u611f\u60c5\u5206\u6790\uff08\u30dd\u30b8\u30c6\u30a3\u30d6/\u30cd\u30ac\u30c6\u30a3\u30d6\uff09</li> <li>\u30c9\u30e1\u30a4\u30f3: \u5546\u54c1\u30ec\u30d3\u30e5\u30fc</li> <li>\u30c7\u30fc\u30bf\u5f62\u5f0f: \u30ec\u30d3\u30e5\u30fc\u30c6\u30ad\u30b9\u30c8\u3068\u8a55\u4fa1\u30b9\u30b3\u30a2</li> </ul>"},{"location":"corpora/#_3","title":"\u57fa\u672c\u7684\u306a\u4f7f\u3044\u65b9","text":""},{"location":"corpora/#_4","title":"\u30b3\u30fc\u30d1\u30b9\u306e\u4e00\u89a7\u3092\u53d6\u5f97","text":"<pre><code>from toiro import datadownloader\n\ncorpora = datadownloader.available_corpus()\nprint(corpora)\n# =&gt; ['livedoor_news_corpus', 'yahoo_movie_reviews', 'amazon_reviews']\n</code></pre>"},{"location":"corpora/#_5","title":"\u30b3\u30fc\u30d1\u30b9\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9","text":"<pre><code>datadownloader.download_corpus(\"livedoor_news_corpus\")\n</code></pre> <p>\u30b3\u30fc\u30d1\u30b9\u306f <code>~/.toiro/</code> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"corpora/#_6","title":"\u30b3\u30fc\u30d1\u30b9\u306e\u8aad\u307f\u8fbc\u307f","text":"<pre><code>train_df, dev_df, test_df = datadownloader.load_corpus(\"livedoor_news_corpus\")\n\n# \u30c7\u30fc\u30bf\u306e\u78ba\u8a8d\nprint(f\"Train: {len(train_df)} samples\")\nprint(f\"Dev: {len(dev_df)} samples\")\nprint(f\"Test: {len(test_df)} samples\")\n\n# \u30c7\u30fc\u30bf\u69cb\u9020\uff08pandas DataFrame\uff09\n# \u30ab\u30e9\u30e0 0: \u30e9\u30d9\u30eb\n# \u30ab\u30e9\u30e0 1: \u30c6\u30ad\u30b9\u30c8\ntexts = train_df[1].tolist()\nlabels = train_df[0].tolist()\n</code></pre>"},{"location":"corpora/#_7","title":"\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406","text":"<p>\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u30b3\u30fc\u30d1\u30b9\u306f\u3001train/dev/test \u306b\u5206\u5272\u6e08\u307f\u306e pandas DataFrame \u3068\u3057\u3066\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002</p> <pre><code># \u4f8b: \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u306e\u8a55\u4fa1\u7528\u306b\u30c6\u30ad\u30b9\u30c8\u306e\u307f\u62bd\u51fa\ntexts = train_df[1].tolist()\n\n# \u4f8b: \u5206\u985e\u5668\u306e\u5b66\u7fd2\u7528\u306b\u30e9\u30d9\u30eb\u3068\u30c6\u30ad\u30b9\u30c8\u3092\u62bd\u51fa\nX_train = train_df[1].tolist()\ny_train = train_df[0].tolist()\n</code></pre> <p>\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u5834\u6240</p> <p>\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u30b3\u30fc\u30d1\u30b9\u306f <code>~/.toiro/</code> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u4fdd\u5b58\u3055\u308c\u307e\u3059\u3002\u4e00\u5ea6\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308c\u3070\u3001\u4ee5\u964d\u306f <code>load_corpus()</code> \u3067\u76f4\u63a5\u8aad\u307f\u8fbc\u3081\u307e\u3059\u3002</p>"},{"location":"installation/","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":""},{"location":"installation/#_2","title":"\u5fc5\u9808\u74b0\u5883","text":"<ul> <li>Python 3.10 \u4ee5\u4e0a</li> </ul>"},{"location":"installation/#_3","title":"\u57fa\u672c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<pre><code>pip install toiro\n</code></pre> <p><code>toiro</code> \u306f\u6700\u5c0f\u69cb\u6210\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f Janome \u304c\u5229\u7528\u53ef\u80fd\u3067\u3059\u3002</p>"},{"location":"installation/#_4","title":"\u8ffd\u52a0\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u306e\u5c0e\u5165","text":"<p>\u6b21\u306e\u3088\u3046\u306b\u500b\u5225\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304f\u3060\u3055\u3044\uff08\u4f8b: SudachiPy \u3068 nagisa \u3092\u8ffd\u52a0\uff09\u3002</p> <pre><code>pip install sudachipy sudachidict_core\npip install nagisa\n</code></pre>"},{"location":"installation/#_5","title":"\u305d\u306e\u4ed6\u306e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6","text":"<pre><code># MeCab (mecab-python3)\npip install mecab-python3\n\n# GiNZA / spaCy \u65e5\u672c\u8a9e\npip install spacy ginza\npip install \"spacy[ja]\"\n\n# KyTea\uff08\u672c\u4f53\u306e\u5c0e\u5165\u304c\u5225\u9014\u5fc5\u8981\uff09\n# \u516c\u5f0f\u624b\u9806\u3067 KyTea \u3092\u5165\u308c\u305f\u4e0a\u3067:\npip install kytea\n\n# Juman++ v2\uff08\u672c\u4f53\u306e\u5c0e\u5165\u304c\u5225\u9014\u5fc5\u8981\uff09\n# \u516c\u5f0f\u624b\u9806\u3067 Juman++ v2 \u3092\u5165\u308c\u305f\u4e0a\u3067:\npip install pyknp\n\n# SentencePiece\npip install sentencepiece\n\n# fugashi + IPADIC / UniDic\npip install fugashi ipadic\npip install fugashi unidic-lite\n\n# TinySegmenter\npip install tinysegmenter3\n\n# tiktoken\uff08GPT-4o / GPT-5 \u7528 BPE\uff09\npip install tiktoken\n</code></pre> <p>\u3059\u3079\u3066\u307e\u3068\u3081\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb</p> <p>\u3059\u3079\u3066\u306e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3092\u4e00\u5ea6\u306b\u8a66\u3057\u305f\u3044\u5834\u5408: <pre><code>pip install \"toiro[all_tokenizers]\"\n</code></pre></p> <p>\u30b7\u30b9\u30c6\u30e0\u30ec\u30d9\u30eb\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5fc5\u8981\u306a\u30c4\u30fc\u30eb</p> <p>KyTea \u3068 Juman++ \u306f\u3001Python \u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u524d\u306b\u30b7\u30b9\u30c6\u30e0\u30ec\u30d9\u30eb\u3067\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5fc5\u8981\u3067\u3059\u3002\u8a73\u7d30\u306f\u5404\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"installation/#docker","title":"Docker \u3092\u4f7f\u3046","text":"<p>\u3059\u3079\u3066\u306e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u304c\u30d7\u30ea\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u305f Docker \u30a4\u30e1\u30fc\u30b8\u3082\u5229\u7528\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>docker run --rm -it taishii/toiro /bin/bash\n</code></pre> <p>\u8a73\u7d30\u306f Docker Hub \u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"installation/#_6","title":"\u5206\u985e\u5668\u306e\u8ffd\u52a0\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09","text":"<p>BERT \u30d9\u30fc\u30b9\u306e\u30c6\u30ad\u30b9\u30c8\u5206\u985e\u5668\u3092\u4f7f\u3044\u305f\u3044\u5834\u5408:</p> <pre><code>pip install \"toiro[all_classifiers]\"\n</code></pre> <p>\u307e\u305f\u306f\u500b\u5225\u306b:</p> <pre><code>pip install torch transformers catalyst\n</code></pre>"},{"location":"license/","title":"\u30e9\u30a4\u30bb\u30f3\u30b9","text":"<p>\u672c\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306f Apache-2.0 License \u306e\u4e0b\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002 \u8a73\u7d30\u306f\u30ea\u30dd\u30b8\u30c8\u30ea\u76f4\u4e0b\u306e <code>LICENSE</code> \u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"quickstart/","title":"\u30af\u30a4\u30c3\u30af\u30b9\u30bf\u30fc\u30c8","text":""},{"location":"quickstart/#_2","title":"\u5229\u7528\u53ef\u80fd\u306a\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u306e\u78ba\u8a8d","text":"<p>\u74b0\u5883\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308b\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3092\u78ba\u8a8d\uff1a</p> <pre><code>from toiro import tokenizers\n\navailable = tokenizers.available_tokenizers()\nprint(available)\n</code></pre> <p>\u51fa\u529b\u4f8b: <pre><code>{\n 'nagisa': {'is_available': True, 'version': '0.2.7'},\n 'janome': {'is_available': True, 'version': '0.3.10'},\n 'mecab-python3': {'is_available': False, 'version': False},\n 'sudachipy': {'is_available': True, 'version': '0.6.8'},\n 'spacy': {'is_available': False, 'version': False},\n 'ginza': {'is_available': False, 'version': False},\n 'kytea': {'is_available': False, 'version': False},\n 'jumanpp': {'is_available': False, 'version': False},\n 'sentencepiece': {'is_available': False, 'version': False},\n 'fugashi-ipadic': {'is_available': False, 'version': False},\n 'fugashi-unidic': {'is_available': False, 'version': False},\n 'tinysegmenter': {'is_available': False, 'version': False},\n 'tiktoken': {'is_available': False, 'version': False}\n}\n</code></pre></p>"},{"location":"quickstart/#_3","title":"\u5206\u304b\u3061\u66f8\u304d\u7d50\u679c\u306e\u6bd4\u8f03","text":"<p>\u540c\u3058\u30c6\u30ad\u30b9\u30c8\u3092\u7570\u306a\u308b\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3067\u5206\u5272\u3057\u3001\u7d50\u679c\u3092\u6bd4\u8f03\uff1a</p> <pre><code>from toiro import tokenizers\n\ntext = \"\u90fd\u5e81\u6240\u5728\u5730\u306f\u65b0\u5bbf\u533a\u3002\"\ntokenizers.print_words(text, delimiter=\"|\")\n</code></pre> <p>\u51fa\u529b\u4f8b: <pre><code>       janome: \u90fd\u5e81|\u6240\u5728\u5730|\u306f|\u65b0\u5bbf|\u533a|\u3002\n       nagisa: \u90fd\u5e81|\u6240\u5728|\u5730|\u306f|\u65b0\u5bbf|\u533a|\u3002\n    sudachipy: \u90fd\u5e81|\u6240\u5728\u5730|\u306f|\u65b0\u5bbf\u533a|\u3002\n</code></pre></p>"},{"location":"quickstart/#_4","title":"\u30b3\u30fc\u30d1\u30b9\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3068\u8aad\u307f\u8fbc\u307f","text":"<p>\u65e5\u672c\u8a9e\u30c6\u30ad\u30b9\u30c8\u5206\u985e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\uff1a</p> <pre><code>from toiro import datadownloader\n\n# \u5229\u7528\u53ef\u80fd\u306a\u30b3\u30fc\u30d1\u30b9\u306e\u78ba\u8a8d\ncorpora = datadownloader.available_corpus()\nprint(corpora)\n# =&gt; ['livedoor_news_corpus', 'yahoo_movie_reviews', 'amazon_reviews']\n\n# \u30b3\u30fc\u30d1\u30b9\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\ncorpus = corpora[0]\ndatadownloader.download_corpus(corpus)\n\n# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\ntrain_df, dev_df, test_df = datadownloader.load_corpus(corpus)\ntexts = train_df[1].tolist()\n</code></pre>"},{"location":"quickstart/#_5","title":"\u901f\u5ea6\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306e\u5b9f\u884c","text":"<p>\u8907\u6570\u306e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3067\u540c\u3058\u30c6\u30ad\u30b9\u30c8\u3092\u51e6\u7406\u3057\u3001\u51e6\u7406\u901f\u5ea6\u3092\u6bd4\u8f03\uff1a</p> <pre><code>from toiro import tokenizers\n\nreport = tokenizers.compare(texts)\nprint(report)\n</code></pre> <p>\u51fa\u529b\u4f8b: <pre><code>[1/3] Tokenizer: janome\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5900/5900 [00:07&lt;00:00, 746.21it/s]\n[2/3] Tokenizer: nagisa\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5900/5900 [00:15&lt;00:00, 370.83it/s]\n[3/3] Tokenizer: sudachipy\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5900/5900 [00:08&lt;00:00, 696.68it/s]\n\n{\n  'execution_environment': {\n    'python_version': '3.10.0',\n    'arch': 'X86_64',\n    'brand_raw': 'Intel(R) Core(TM) i7-7700K CPU @ 4.20GHz',\n    'count': 8\n  },\n  'data': {\n    'number_of_sentences': 5900,\n    'average_length': 37.69\n  },\n  'janome': {'elapsed_time': 9.11},\n  'nagisa': {'elapsed_time': 15.87},\n  'sudachipy': {'elapsed_time': 9.05}\n}\n</code></pre></p>"},{"location":"quickstart/#_6","title":"\u6b21\u306e\u30b9\u30c6\u30c3\u30d7","text":"<ul> <li>\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u4e00\u89a7 - \u5bfe\u5fdc\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u306e\u8a73\u7d30</li> <li>\u30d9\u30f3\u30c1\u30de\u30fc\u30af - \u3088\u308a\u8a73\u7d30\u306a\u6bd4\u8f03\u65b9\u6cd5</li> <li>API \u30ea\u30d5\u30a1\u30ec\u30f3\u30b9 - \u5b8c\u5168\u306a API \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8</li> </ul>"},{"location":"tokenizers/","title":"\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u4e00\u89a7","text":"<p>toiro \u3067\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u308b\u65e5\u672c\u8a9e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u3068 BPE \u30e2\u30c7\u30eb\u306e\u4e00\u89a7\u3067\u3059\u3002</p>"},{"location":"tokenizers/#_2","title":"\u5f62\u614b\u7d20\u89e3\u6790\u30d9\u30fc\u30b9\u306e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6","text":""},{"location":"tokenizers/#janome","title":"Janome","text":"<ul> <li>\u7a2e\u985e: \u5f62\u614b\u7d20\u89e3\u6790</li> <li>\u8f9e\u66f8: MeCab IPADIC</li> <li>\u7279\u5fb4: Pure Python \u5b9f\u88c5\u3001\u5916\u90e8\u4f9d\u5b58\u306a\u3057</li> <li>\u30c7\u30d5\u30a9\u30eb\u30c8: toiro \u306b\u6a19\u6e96\u3067\u542b\u307e\u308c\u308b</li> </ul>"},{"location":"tokenizers/#nagisa","title":"nagisa","text":"<ul> <li>\u7a2e\u985e: RNN \u30d9\u30fc\u30b9</li> <li>\u7279\u5fb4: \u54c1\u8a5e\u30bf\u30b0\u4ed8\u3051\u3068\u56fa\u6709\u8868\u73fe\u62bd\u51fa\u3082\u30b5\u30dd\u30fc\u30c8</li> </ul>"},{"location":"tokenizers/#mecab-python3","title":"mecab-python3","text":"<ul> <li>\u7a2e\u985e: \u5f62\u614b\u7d20\u89e3\u6790</li> <li>\u8f9e\u66f8: MeCab IPADIC</li> <li>\u7279\u5fb4: MeCab \u306e Python \u30d0\u30a4\u30f3\u30c7\u30a3\u30f3\u30b0</li> </ul>"},{"location":"tokenizers/#sudachipy","title":"SudachiPy","text":"<ul> <li>\u7a2e\u985e: \u5f62\u614b\u7d20\u89e3\u6790</li> <li>\u8f9e\u66f8: Sudachi \u8f9e\u66f8</li> <li>\u7279\u5fb4: \u8907\u6570\u306e\u5206\u5272\u30e2\u30fc\u30c9\uff08A/B/C\uff09\u3001\u540c\u7fa9\u8a9e\u5c55\u958b</li> </ul>"},{"location":"tokenizers/#spacy","title":"spaCy","text":"<ul> <li>\u7a2e\u985e: \u7d71\u8a08\u30e2\u30c7\u30eb\u30d9\u30fc\u30b9</li> <li>\u7279\u5fb4: \u56fa\u6709\u8868\u73fe\u8a8d\u8b58\u3001\u4f9d\u5b58\u69cb\u9020\u89e3\u6790\u306a\u3069\u591a\u6a5f\u80fd</li> </ul>"},{"location":"tokenizers/#ginza","title":"GiNZA","text":"<ul> <li>\u7a2e\u985e: spaCy \u306e\u65e5\u672c\u8a9e\u30e2\u30c7\u30eb</li> <li>\u7279\u5fb4: Universal Dependencies \u6e96\u62e0\u3001\u56fa\u6709\u8868\u73fe\u8a8d\u8b58</li> </ul>"},{"location":"tokenizers/#kytea","title":"KyTea","text":"<ul> <li>\u7a2e\u985e: \u70b9\u4e88\u6e2c\u30d9\u30fc\u30b9</li> <li>\u7279\u5fb4: \u8aad\u307f\u63a8\u5b9a\u6a5f\u80fd</li> <li>\u6ce8\u610f: \u30b7\u30b9\u30c6\u30e0\u30ec\u30d9\u30eb\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5fc5\u8981</li> </ul>"},{"location":"tokenizers/#juman","title":"Juman++","text":"<ul> <li>\u7a2e\u985e: \u5f62\u614b\u7d20\u89e3\u6790</li> <li>\u8f9e\u66f8: JUMAN \u8f9e\u66f8</li> <li>\u7279\u5fb4: RNN \u306b\u3088\u308b\u518d\u9806\u4f4d\u4ed8\u3051</li> <li>\u6ce8\u610f: \u30b7\u30b9\u30c6\u30e0\u30ec\u30d9\u30eb\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5fc5\u8981\uff08pyknp \u7d4c\u7531\u3067\u4f7f\u7528\uff09</li> </ul>"},{"location":"tokenizers/#fugashi","title":"fugashi","text":"<ul> <li>\u7a2e\u985e: MeCab \u306e Cython \u30e9\u30c3\u30d1\u30fc</li> <li>\u8f9e\u66f8: IPADIC \u307e\u305f\u306f UniDic</li> <li>\u7279\u5fb4: \u9ad8\u901f\u306a MeCab Python \u30d0\u30a4\u30f3\u30c7\u30a3\u30f3\u30b0</li> </ul>"},{"location":"tokenizers/#tinysegmenter","title":"TinySegmenter","text":"<ul> <li>\u7a2e\u985e: \u30b3\u30f3\u30d1\u30af\u30c8\u306a\u5206\u304b\u3061\u66f8\u304d</li> <li>\u7279\u5fb4: \u8efd\u91cf\u3001\u8f9e\u66f8\u4e0d\u8981</li> </ul>"},{"location":"tokenizers/#_3","title":"\u30b5\u30d6\u30ef\u30fc\u30c9\u30c8\u30fc\u30af\u30ca\u30a4\u30b6","text":""},{"location":"tokenizers/#sentencepiece","title":"SentencePiece","text":"<ul> <li>\u7a2e\u985e: BPE / Unigram</li> <li>\u7279\u5fb4: \u8a00\u8a9e\u975e\u4f9d\u5b58\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u6a5f\u68b0\u7ffb\u8a33\u5411\u3051</li> </ul>"},{"location":"tokenizers/#tiktoken","title":"tiktoken","text":"<ul> <li>\u7a2e\u985e: BPE</li> <li>\u30e2\u30c7\u30eb: GPT-4o / GPT-5</li> <li>\u7279\u5fb4: OpenAI \u30e2\u30c7\u30eb\u7528\u30c8\u30fc\u30af\u30ca\u30a4\u30b6</li> </ul>"},{"location":"tokenizers/#_4","title":"\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u306e\u9078\u3073\u65b9","text":"\u7528\u9014 \u304a\u3059\u3059\u3081 \u624b\u8efd\u306b\u59cb\u3081\u305f\u3044 Janome\uff08\u4f9d\u5b58\u306a\u3057\uff09 \u9ad8\u901f\u51e6\u7406 MeCab, fugashi, SudachiPy \u56fa\u6709\u8868\u73fe\u8a8d\u8b58\u3082\u5fc5\u8981 GiNZA, spaCy \u30cb\u30e5\u30fc\u30e9\u30eb\u6a5f\u68b0\u7ffb\u8a33 SentencePiece OpenAI \u30e2\u30c7\u30eb\u3068\u7d71\u5408 tiktoken <p>\u30b7\u30b9\u30c6\u30e0\u30ec\u30d9\u30eb\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5fc5\u8981</p> <p>KyTea \u3068 Juman++ \u306f\u3001Python \u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u524d\u306b\u30b7\u30b9\u30c6\u30e0\u30ec\u30d9\u30eb\u3067\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5fc5\u8981\u3067\u3059\u3002\u8a73\u7d30\u306f\u5404\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"en/","title":"toiro Documentation","text":"<p>toiro is a Python package for comparing Japanese tokenizers. You can:</p> <ul> <li>Compare processing speed across tokenizers</li> <li>Compare tokenization outputs side by side</li> <li>Evaluate downstream task performance (e.g., text classification)</li> <li>Use helper utilities for Japanese NLP (corpus download/preprocessing, simple classifiers, etc.)</li> </ul> <p></p>"},{"location":"en/#key-features","title":"Key Features","text":""},{"location":"en/#supported-tokenizers","title":"Supported Tokenizers","text":"<p>13 Japanese tokenizers and BPE models:</p> <ul> <li>janome (included by default)</li> <li>nagisa</li> <li>mecab-python3</li> <li>sudachipy</li> <li>spacy</li> <li>ginza</li> <li>kytea</li> <li>jumanpp</li> <li>sentencepiece</li> <li>fugashi (ipadic/unidic)</li> <li>tinysegmenter</li> <li>tiktoken (BPE for GPT-4o / GPT-5)</li> </ul>"},{"location":"en/#links","title":"Links","text":"<p>\ud83d\udc49 Project: https://github.com/taishi-i/toiro \ud83d\udc49 Demo (Hugging Face Spaces): https://huggingface.co/spaces/taishi-i/Japanese-Tokenizer-Comparison \ud83d\udc49 PyPI: https://pypi.org/project/toiro/</p> <p>Supported Python Versions</p> <p>Python 3.10 or later is recommended.</p> <p>License</p> <p>toiro is released under the Apache License 2.0.</p> <p>This documentation site is generated with MkDocs Material.</p>"},{"location":"en/api/","title":"API Reference","text":"<p>Auto-generated API docs:</p>"},{"location":"en/benchmarks/","title":"Benchmarks","text":"<p>toiro provides utilities to compare tokenizers from three perspectives.</p>"},{"location":"en/benchmarks/#1-speed-comparison","title":"1. Speed Comparison","text":"<p>Process the same texts with multiple tokenizers and measure execution time.</p> <pre><code>from toiro import tokenizers\nfrom toiro import datadownloader\n\n# Download a corpus\ndatadownloader.download_corpus(\"livedoor_news_corpus\")\ntrain_df, dev_df, test_df = datadownloader.load_corpus(\"livedoor_news_corpus\")\ntexts = train_df[1].tolist()\n\n# Run speed comparison\nreport = tokenizers.compare(texts)\nprint(report)\n</code></pre> <p>Example output: <pre><code>{\n  'execution_environment': {\n    'python_version': '3.10.0',\n    'arch': 'X86_64',\n    'brand_raw': 'Intel(R) Core(TM) i7-7700K CPU @ 4.20GHz',\n    'count': 8\n  },\n  'data': {\n    'number_of_sentences': 5900,\n    'average_length': 37.69\n  },\n  'janome': {'elapsed_time': 9.11},\n  'nagisa': {'elapsed_time': 15.87},\n  'sudachipy': {'elapsed_time': 9.05}\n}\n</code></pre></p>"},{"location":"en/benchmarks/#2-segmentation-comparison","title":"2. Segmentation Comparison","text":"<p>Visually compare how each tokenizer segments the same text.</p> <pre><code>from toiro import tokenizers\n\ntext = \"\u90fd\u5e81\u6240\u5728\u5730\u306f\u65b0\u5bbf\u533a\u3002\"\ntokenizers.print_words(text, delimiter=\"|\")\n</code></pre> <p>Example output: <pre><code>       janome: \u90fd\u5e81|\u6240\u5728\u5730|\u306f|\u65b0\u5bbf|\u533a|\u3002\n       nagisa: \u90fd\u5e81|\u6240\u5728|\u5730|\u306f|\u65b0\u5bbf|\u533a|\u3002\n    sudachipy: \u90fd\u5e81|\u6240\u5728\u5730|\u306f|\u65b0\u5bbf\u533a|\u3002\n</code></pre></p>"},{"location":"en/benchmarks/#detailed-comparison","title":"Detailed Comparison","text":"<p>You can also select a specific tokenizer for detailed analysis:</p> <pre><code>from toiro.tokenizers import SelectTokenizer\n\n# Select a specific tokenizer\ntokenizer = SelectTokenizer(\"sudachipy\")\ntokens = tokenizer.tokenize(\"\u90fd\u5e81\u6240\u5728\u5730\u306f\u65b0\u5bbf\u533a\u3002\")\nprint(tokens)\n</code></pre>"},{"location":"en/benchmarks/#3-application-task-performance","title":"3. Application Task Performance","text":"<p>Evaluate tokenizer performance on real application tasks (e.g., text classification).</p>"},{"location":"en/benchmarks/#classification-with-svm","title":"Classification with SVM","text":"<pre><code>from toiro import tokenizers, datadownloader\nfrom toiro.classifiers import SVMClassificationModel\n\n# Prepare data\ntrain_df, dev_df, test_df = datadownloader.load_corpus(\"livedoor_news_corpus\")\n\n# Select a tokenizer\ntokenizer = tokenizers.SelectTokenizer(\"janome\")\n\n# Train and evaluate with SVM\nmodel = SVMClassificationModel(tokenizer)\nmodel.train(train_df)\nmetrics = model.evaluate(test_df)\nprint(metrics)\n</code></pre>"},{"location":"en/benchmarks/#classification-with-bert","title":"Classification with BERT","text":"<pre><code>from toiro.classifiers import BERTClassificationModel\n\n# Train and evaluate with BERT (requires: torch, transformers, catalyst)\nmodel = BERTClassificationModel(\"cl-tohoku/bert-base-japanese\")\nmodel.train(train_df, dev_df, epochs=3)\nmetrics = model.evaluate(test_df)\nprint(metrics)\n</code></pre>"},{"location":"en/benchmarks/#saving-benchmark-results","title":"Saving Benchmark Results","text":"<p>You can also compare directly from a file:</p> <pre><code>from toiro import tokenizers\n\n# Compare from file\nreport = tokenizers.compare_from_file(\"path/to/textfile.txt\")\nprint(report)\n</code></pre> <p>Performance Tips</p> <ul> <li>For large text volumes, MeCab, fugashi, and SudachiPy are fastest</li> <li>For memory efficiency, Janome or TinySegmenter are recommended</li> <li>For accuracy-focused tasks, consider GiNZA or BERT-based models</li> </ul> <p>Detailed Examples</p> <p>For more detailed usage examples, see the examples directory on GitHub.</p>"},{"location":"en/contributing/","title":"Contributing Guide","text":"<p>We welcome contributions to the toiro project! Whether it's bug reports, feature requests, or pull requests, all forms of contribution are appreciated.</p>"},{"location":"en/contributing/#code-of-conduct-and-guidelines","title":"Code of Conduct and Guidelines","text":"<ul> <li>Code of Conduct: CODE_OF_CONDUCT.md</li> <li>Contributing Guide: CONTRIBUTING.md</li> </ul>"},{"location":"en/contributing/#development-setup","title":"Development Setup","text":""},{"location":"en/contributing/#1-clone-the-repository","title":"1. Clone the repository","text":"<pre><code>git clone https://github.com/taishi-i/toiro.git\ncd toiro\n</code></pre>"},{"location":"en/contributing/#2-create-and-activate-a-virtual-environment","title":"2. Create and activate a virtual environment","text":"<pre><code>python -m venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\n</code></pre>"},{"location":"en/contributing/#3-install-development-packages","title":"3. Install development packages","text":"<pre><code>pip install -U pip\npip install -e .\npip install -e \".[all_tokenizers]\"  # Install all tokenizers (optional)\n</code></pre>"},{"location":"en/contributing/#4-run-tests","title":"4. Run tests","text":"<pre><code>pytest -q  # or: python -m pytest\n</code></pre> <p>Test files are located in the <code>test/</code> directory: - <code>test_tokenizers.py</code> - Tokenizer tests - <code>test_datadownloader.py</code> - Data downloader tests - <code>test_classifiers.py</code> - Classifier tests</p>"},{"location":"en/contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"en/contributing/#bug-reports","title":"Bug Reports","text":"<p>Report bugs via GitHub Issues. Please include: - Python version - toiro version - Steps to reproduce - Error messages</p>"},{"location":"en/contributing/#feature-requests","title":"Feature Requests","text":"<p>If you have ideas for new features, propose them via GitHub Issues.</p>"},{"location":"en/contributing/#pull-requests","title":"Pull Requests","text":"<ol> <li>Fork the repository</li> <li>Create a new branch (<code>git checkout -b feature/your-feature</code>)</li> <li>Commit your changes (<code>git commit -am 'Add some feature'</code>)</li> <li>Push to the branch (<code>git push origin feature/your-feature</code>)</li> <li>Create a pull request</li> </ol>"},{"location":"en/contributing/#adding-a-tokenizer","title":"Adding a Tokenizer","text":"<p>To add a new tokenizer:</p> <ol> <li>Create a new file in <code>toiro/tokenizers/</code> (e.g., <code>tokenizer_newone.py</code>)</li> <li>Implement the <code>tokenize()</code> function</li> <li>Add an availability check function in <code>tokenizer_utils.py</code></li> <li>Update <code>__init__.py</code> to import the new tokenizer</li> <li>Add tests in <code>test/test_tokenizers.py</code></li> </ol>"},{"location":"en/contributing/#updating-documentation","title":"Updating Documentation","text":"<p>Documentation is in the <code>docs/</code> directory. You can preview it locally using MkDocs:</p> <pre><code>pip install mkdocs-material mkdocs-static-i18n mkdocstrings[python]\nmkdocs serve\n</code></pre> <p>Open <code>http://127.0.0.1:8000</code> in your browser to view the preview.</p>"},{"location":"en/contributing/#questions-and-support","title":"Questions and Support","text":"<p>If you have questions, feel free to ask via GitHub Discussions or Issues.</p>"},{"location":"en/corpora/","title":"Corpora &amp; Data","text":"<p>The <code>toiro.datadownloader</code> module provides easy access to Japanese text classification corpora.</p>"},{"location":"en/corpora/#available-corpora","title":"Available Corpora","text":""},{"location":"en/corpora/#livedoor-news-corpus","title":"livedoor News Corpus","text":"<ul> <li>Task: Text classification (news category)</li> <li>Categories: 9 categories</li> <li>Categories: Topic News, Sports Watch, IT Life Hack, Kaden Channel, MOVIE ENTER, Dokujo Tsushin, S-MAX, livedoor HOMME, Peachy</li> <li>Source: livedoor News Corpus</li> </ul>"},{"location":"en/corpora/#yahoo-movie-reviews","title":"Yahoo! Movie Reviews","text":"<ul> <li>Task: Sentiment analysis (positive/negative)</li> <li>Domain: Movie reviews</li> <li>Format: Review text and rating scores</li> </ul>"},{"location":"en/corpora/#amazon-reviews","title":"Amazon Reviews","text":"<ul> <li>Task: Sentiment analysis (positive/negative)</li> <li>Domain: Product reviews</li> <li>Format: Review text and rating scores</li> </ul>"},{"location":"en/corpora/#basic-usage","title":"Basic Usage","text":""},{"location":"en/corpora/#list-available-corpora","title":"List available corpora","text":"<pre><code>from toiro import datadownloader\n\ncorpora = datadownloader.available_corpus()\nprint(corpora)\n# =&gt; ['livedoor_news_corpus', 'yahoo_movie_reviews', 'amazon_reviews']\n</code></pre>"},{"location":"en/corpora/#download-a-corpus","title":"Download a corpus","text":"<pre><code>datadownloader.download_corpus(\"livedoor_news_corpus\")\n</code></pre> <p>Corpora are downloaded to the <code>~/.toiro/</code> directory.</p>"},{"location":"en/corpora/#load-a-corpus","title":"Load a corpus","text":"<pre><code>train_df, dev_df, test_df = datadownloader.load_corpus(\"livedoor_news_corpus\")\n\n# Check data\nprint(f\"Train: {len(train_df)} samples\")\nprint(f\"Dev: {len(dev_df)} samples\")\nprint(f\"Test: {len(test_df)} samples\")\n\n# Data structure (pandas DataFrame)\n# Column 0: label\n# Column 1: text\ntexts = train_df[1].tolist()\nlabels = train_df[0].tolist()\n</code></pre>"},{"location":"en/corpora/#data-preprocessing","title":"Data Preprocessing","text":"<p>Downloaded corpora are provided as pandas DataFrames, pre-split into train/dev/test sets.</p> <pre><code># Example: Extract texts only for tokenizer evaluation\ntexts = train_df[1].tolist()\n\n# Example: Extract labels and texts for classifier training\nX_train = train_df[1].tolist()\ny_train = train_df[0].tolist()\n</code></pre> <p>Data storage location</p> <p>Downloaded corpora are saved in the <code>~/.toiro/</code> directory. Once downloaded, you can load them directly with <code>load_corpus()</code>.</p>"},{"location":"en/installation/","title":"Installation","text":""},{"location":"en/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or later</li> </ul>"},{"location":"en/installation/#basic-install","title":"Basic install","text":"<pre><code>pip install toiro\n</code></pre> <p><code>toiro</code> ships with a minimal setup, and Janome is available by default.</p>"},{"location":"en/installation/#add-extra-tokenizers","title":"Add extra tokenizers","text":"<p>Install additional tokenizers individually, e.g., SudachiPy and nagisa:</p> <pre><code>pip install sudachipy sudachidict_core\npip install nagisa\n</code></pre>"},{"location":"en/installation/#other-tokenizers","title":"Other tokenizers","text":"<pre><code># MeCab (mecab-python3)\npip install mecab-python3\n\n# spaCy Japanese / GiNZA\npip install spacy ginza\npip install \"spacy[ja]\"\n\n# KyTea (requires system install)\n# After installing KyTea by its official instructions:\npip install kytea\n\n# Juman++ v2 (requires system install)\n# After installing Juman++ v2 by its official instructions:\npip install pyknp\n\n# SentencePiece\npip install sentencepiece\n\n# fugashi + IPADIC / UniDic\npip install fugashi ipadic\npip install fugashi unidic-lite\n\n# TinySegmenter\npip install tinysegmenter3\n\n# tiktoken (BPE for GPT-4o / GPT-5)\npip install tiktoken\n</code></pre> <p>Install all at once</p> <p>To try all tokenizers at once: <pre><code>pip install \"toiro[all_tokenizers]\"\n</code></pre></p> <p>System-level installation required</p> <p>KyTea and Juman++ require system-level installation before installing the Python package. Please refer to their official documentation for details.</p>"},{"location":"en/installation/#using-docker","title":"Using Docker","text":"<p>A Docker image with all tokenizers pre-installed is available:</p> <pre><code>docker run --rm -it taishii/toiro /bin/bash\n</code></pre> <p>See Docker Hub for details.</p>"},{"location":"en/installation/#add-classifiers-optional","title":"Add classifiers (optional)","text":"<p>To use BERT-based text classifiers:</p> <pre><code>pip install \"toiro[all_classifiers]\"\n</code></pre> <p>Or install individually:</p> <pre><code>pip install torch transformers catalyst\n</code></pre>"},{"location":"en/license/","title":"License","text":"<p>This project is released under the Apache-2.0 License. See <code>LICENSE</code> at the repository root for details.</p>"},{"location":"en/quickstart/","title":"Quickstart","text":""},{"location":"en/quickstart/#check-available-tokenizers","title":"Check available tokenizers","text":"<p>Check which tokenizers are installed in your environment:</p> <pre><code>from toiro import tokenizers\n\navailable = tokenizers.available_tokenizers()\nprint(available)\n</code></pre> <p>Example output: <pre><code>{\n 'nagisa': {'is_available': True, 'version': '0.2.7'},\n 'janome': {'is_available': True, 'version': '0.3.10'},\n 'mecab-python3': {'is_available': False, 'version': False},\n 'sudachipy': {'is_available': True, 'version': '0.6.8'},\n 'spacy': {'is_available': False, 'version': False},\n 'ginza': {'is_available': False, 'version': False},\n 'kytea': {'is_available': False, 'version': False},\n 'jumanpp': {'is_available': False, 'version': False},\n 'sentencepiece': {'is_available': False, 'version': False},\n 'fugashi-ipadic': {'is_available': False, 'version': False},\n 'fugashi-unidic': {'is_available': False, 'version': False},\n 'tinysegmenter': {'is_available': False, 'version': False},\n 'tiktoken': {'is_available': False, 'version': False}\n}\n</code></pre></p>"},{"location":"en/quickstart/#compare-segmentation-outputs","title":"Compare segmentation outputs","text":"<p>Tokenize the same text with different tokenizers and compare results:</p> <pre><code>from toiro import tokenizers\n\ntext = \"\u90fd\u5e81\u6240\u5728\u5730\u306f\u65b0\u5bbf\u533a\u3002\"\ntokenizers.print_words(text, delimiter=\"|\")\n</code></pre> <p>Example output: <pre><code>       janome: \u90fd\u5e81|\u6240\u5728\u5730|\u306f|\u65b0\u5bbf|\u533a|\u3002\n       nagisa: \u90fd\u5e81|\u6240\u5728|\u5730|\u306f|\u65b0\u5bbf|\u533a|\u3002\n    sudachipy: \u90fd\u5e81|\u6240\u5728\u5730|\u306f|\u65b0\u5bbf\u533a|\u3002\n</code></pre></p>"},{"location":"en/quickstart/#download-and-load-a-corpus","title":"Download and load a corpus","text":"<p>Download a Japanese text classification dataset:</p> <pre><code>from toiro import datadownloader\n\n# Check available corpora\ncorpora = datadownloader.available_corpus()\nprint(corpora)\n# =&gt; ['livedoor_news_corpus', 'yahoo_movie_reviews', 'amazon_reviews']\n\n# Download a corpus\ncorpus = corpora[0]\ndatadownloader.download_corpus(corpus)\n\n# Load the data\ntrain_df, dev_df, test_df = datadownloader.load_corpus(corpus)\ntexts = train_df[1].tolist()\n</code></pre>"},{"location":"en/quickstart/#run-a-speed-benchmark","title":"Run a speed benchmark","text":"<p>Process the same texts with multiple tokenizers and compare their speed:</p> <pre><code>from toiro import tokenizers\n\nreport = tokenizers.compare(texts)\nprint(report)\n</code></pre> <p>Example output: <pre><code>[1/3] Tokenizer: janome\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5900/5900 [00:07&lt;00:00, 746.21it/s]\n[2/3] Tokenizer: nagisa\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5900/5900 [00:15&lt;00:00, 370.83it/s]\n[3/3] Tokenizer: sudachipy\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5900/5900 [00:08&lt;00:00, 696.68it/s]\n\n{\n  'execution_environment': {\n    'python_version': '3.10.0',\n    'arch': 'X86_64',\n    'brand_raw': 'Intel(R) Core(TM) i7-7700K CPU @ 4.20GHz',\n    'count': 8\n  },\n  'data': {\n    'number_of_sentences': 5900,\n    'average_length': 37.69\n  },\n  'janome': {'elapsed_time': 9.11},\n  'nagisa': {'elapsed_time': 15.87},\n  'sudachipy': {'elapsed_time': 9.05}\n}\n</code></pre></p>"},{"location":"en/quickstart/#next-steps","title":"Next steps","text":"<ul> <li>Tokenizers - Details about supported tokenizers</li> <li>Benchmarks - More detailed comparison methods</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"en/tokenizers/","title":"Tokenizers","text":"<p>List of Japanese tokenizers and BPE models supported by toiro.</p>"},{"location":"en/tokenizers/#morphological-analysis-tokenizers","title":"Morphological Analysis Tokenizers","text":""},{"location":"en/tokenizers/#janome","title":"Janome","text":"<ul> <li>Type: Morphological analyzer</li> <li>Dictionary: MeCab IPADIC</li> <li>Features: Pure Python implementation, no external dependencies</li> <li>Default: Included in toiro by default</li> </ul>"},{"location":"en/tokenizers/#nagisa","title":"nagisa","text":"<ul> <li>Type: RNN-based</li> <li>Features: Supports POS tagging and named entity extraction</li> </ul>"},{"location":"en/tokenizers/#mecab-python3","title":"mecab-python3","text":"<ul> <li>Type: Morphological analyzer</li> <li>Dictionary: MeCab IPADIC</li> <li>Features: Python binding for MeCab</li> </ul>"},{"location":"en/tokenizers/#sudachipy","title":"SudachiPy","text":"<ul> <li>Type: Morphological analyzer</li> <li>Dictionary: Sudachi dictionary</li> <li>Features: Multiple split modes (A/B/C), synonym expansion</li> </ul>"},{"location":"en/tokenizers/#spacy","title":"spaCy","text":"<ul> <li>Type: Statistical model-based</li> <li>Features: Multi-functional including NER, dependency parsing</li> </ul>"},{"location":"en/tokenizers/#ginza","title":"GiNZA","text":"<ul> <li>Type: Japanese model for spaCy</li> <li>Features: Universal Dependencies compliant, NER</li> </ul>"},{"location":"en/tokenizers/#kytea","title":"KyTea","text":"<ul> <li>Type: Pointwise prediction-based</li> <li>Features: Pronunciation estimation</li> <li>Note: Requires system-level installation</li> </ul>"},{"location":"en/tokenizers/#juman","title":"Juman++","text":"<ul> <li>Type: Morphological analyzer</li> <li>Dictionary: JUMAN dictionary</li> <li>Features: RNN-based re-ranking</li> <li>Note: Requires system-level installation (used via pyknp)</li> </ul>"},{"location":"en/tokenizers/#fugashi","title":"fugashi","text":"<ul> <li>Type: Cython wrapper for MeCab</li> <li>Dictionary: IPADIC or UniDic</li> <li>Features: Fast MeCab Python binding</li> </ul>"},{"location":"en/tokenizers/#tinysegmenter","title":"TinySegmenter","text":"<ul> <li>Type: Compact tokenizer</li> <li>Features: Lightweight, dictionary-free</li> </ul>"},{"location":"en/tokenizers/#subword-tokenizers","title":"Subword Tokenizers","text":""},{"location":"en/tokenizers/#sentencepiece","title":"SentencePiece","text":"<ul> <li>Type: BPE / Unigram</li> <li>Features: Language-independent, designed for neural machine translation</li> </ul>"},{"location":"en/tokenizers/#tiktoken","title":"tiktoken","text":"<ul> <li>Type: BPE</li> <li>Models: GPT-4o / GPT-5</li> <li>Features: Tokenizer for OpenAI models</li> </ul>"},{"location":"en/tokenizers/#choosing-a-tokenizer","title":"Choosing a Tokenizer","text":"Use Case Recommendation Easy to start Janome (no dependencies) High-speed processing MeCab, fugashi, SudachiPy Need NER GiNZA, spaCy Neural machine translation SentencePiece Integration with OpenAI models tiktoken <p>System-level installation required</p> <p>KyTea and Juman++ require system-level installation before installing the Python package. Please refer to their official documentation for details.</p>"}]}